{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_trees = [1,5,10,20,25,50,100,200,500,1000]\n",
    "#csv_files = ['generated_csvs/dataset_191_wine-metrics.csv', 'generated_csvs/dataset_31_credit-g-metrics_ate500.csv', 'generated_csvs/phpOkU53r-metrics.csv']\n",
    "#dataset_names = ['Wine dataset(dataset_191_wine)','German Credit Dataset(dataset_31_credit_g)', 'Vertebral column dataset (phpOkU53r)']\n",
    "\n",
    "csv_files = ['generated_csvs/phpOkU53r-accuracy-greater-50-metrics.csv', 'generated_csvs/dataset_191_wine-accuracy-greater-50-metrics.csv']\n",
    "dataset_names = ['Vertebral column dataset (phpOkU53r)-Greater 50', 'Wine dataset(dataset_191_wine-Greater 50)']\n",
    "\n",
    "def read_csv(csv_path, attributes_to_obtain=None):\n",
    "    with open(csv_path) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        csv_info = list(reader)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(csv_info[1:], columns=csv_info[0])\n",
    "    if attributes_to_obtain is not None:\n",
    "        df = df[attributes_to_obtain]\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getMeanMetrics(df):\n",
    "    mean_metrics = []\n",
    "    std_metrics = []\n",
    "    \n",
    "    for n_tree in n_trees:\n",
    "        n_tree_dataset = df[df['n_trees'] == n_tree]\n",
    "    \n",
    "        mean_accuracy,std_accuracy = np.mean(n_tree_dataset['accuracy'].values), np.std(n_tree_dataset['accuracy'].values)\n",
    "        mean_recall,std_recall = np.mean(n_tree_dataset['mean_recall'].values), np.std(n_tree_dataset['mean_recall'].values)\n",
    "        mean_prec, std_prec = np.mean(n_tree_dataset['mean_precision'].values), np.std(n_tree_dataset['mean_precision'].values)\n",
    "        mean_F1, std_F1 = np.mean(n_tree_dataset['mean_F1'].values), np.std(n_tree_dataset['mean_F1'].values)\n",
    "\n",
    "        mean_metrics.append([n_tree] + [mean_accuracy, mean_recall, mean_prec, mean_F1])\n",
    "        std_metrics.append([n_tree] + [std_accuracy, std_recall, std_prec, std_F1])\n",
    "        \n",
    "    return mean_metrics, std_metrics\n",
    "\n",
    "def getMeanMetricsPerCV(df):\n",
    "    mean_metrics = []\n",
    "    std_metrics = []\n",
    "    \n",
    "    for n_tree in n_trees:\n",
    "        mean_metrics_tree = []\n",
    "        std_metrics_tree = []\n",
    "        \n",
    "        n_tree_dataset = df[df['n_trees'] == n_tree]\n",
    "        \n",
    "        for cv in range(1,11):\n",
    "            n_tree_cv_dataset = n_tree_dataset[n_tree_dataset['cross_val'] == cv]\n",
    "            \n",
    "            acc_cv,std_acc = np.mean(n_tree_cv_dataset['accuracy'].values), np.std(n_tree_cv_dataset['accuracy'].values)\n",
    "            rec_cv,std_rec = np.mean(n_tree_cv_dataset['mean_recall'].values), np.std(n_tree_cv_dataset['mean_recall'].values)\n",
    "            prec_cv, std_prec = np.mean(n_tree_cv_dataset['mean_precision'].values), np.std(n_tree_cv_dataset['mean_precision'].values)\n",
    "            F1_cv, std_F1 = np.mean(n_tree_cv_dataset['mean_F1'].values), np.std(n_tree_cv_dataset['mean_F1'].values)\n",
    "            \n",
    "            mean_metrics_tree.append([cv,n_tree] + [acc_cv, rec_cv, prec_cv, F1_cv])\n",
    "            std_metrics_tree.append([cv, n_tree] + [std_acc, std_rec, std_prec, std_F1])\n",
    "            \n",
    "        mean_metrics.append(mean_metrics_tree)\n",
    "        std_metrics.append(std_metrics_tree)\n",
    "            \n",
    "    return mean_metrics, std_metrics\n",
    "\n",
    "def getPrecisionRecallPerClass(df, number_of_classes):\n",
    "    prec_f1_recalls = []\n",
    "    \n",
    "    for n_tree in n_trees:\n",
    "        prec_f1_recall = []\n",
    "        n_tree_dataset = df[df['n_trees'] == n_tree]\n",
    "        \n",
    "        if number_of_classes == 2:\n",
    "            class_1_recall = np.mean(n_tree_dataset['class_good_recall'].values)\n",
    "            class_1_prec = np.mean(n_tree_dataset['class_good_precision'].values)\n",
    "            class_1_f1 = np.mean(n_tree_dataset['class_good_F1'].values)\n",
    "\n",
    "            class_2_recall = np.mean(n_tree_dataset['class_bad_recall'].values)\n",
    "            class_2_prec = np.mean(n_tree_dataset['class_bad_precision'].values)\n",
    "            class_2_f1 = np.mean(n_tree_dataset['class_bad_F1'].values)\n",
    "        \n",
    "            prec_f1_recalls.append([n_tree] + [class_1_recall, class_1_prec, class_1_f1, \n",
    "                                            class_2_recall, class_2_prec, class_2_f1])\n",
    "        \n",
    "        elif number_of_classes == 3:\n",
    "            class_1_recall = np.mean(n_tree_dataset['class_1_recall'].values)\n",
    "            class_1_prec = np.mean(n_tree_dataset['class_1_precision'].values)\n",
    "            class_1_f1 = np.mean(n_tree_dataset['class_1_F1'].values)\n",
    "\n",
    "            class_2_recall = np.mean(n_tree_dataset['class_2_recall'].values)\n",
    "            class_2_prec = np.mean(n_tree_dataset['class_2_precision'].values)\n",
    "            class_2_f1 = np.mean(n_tree_dataset['class_2_F1'].values)\n",
    "            \n",
    "            class_3_recall = np.mean(n_tree_dataset['class_3_recall'].values)\n",
    "            class_3_prec = np.mean(n_tree_dataset['class_3_precision'].values)\n",
    "            class_3_f1 = np.mean(n_tree_dataset['class_3_F1'].values)\n",
    "            \n",
    "            prec_f1_recalls.append([n_tree] + [class_1_recall, class_1_prec, class_1_f1, \n",
    "                                                class_2_recall, class_2_prec, class_2_f1,\n",
    "                                              class_3_recall, class_3_prec, class_3_f1])\n",
    "        \n",
    "    return prec_f1_recalls\n",
    "            \n",
    "        \n",
    "\n",
    "def plot_mean_std(x_axis, y_axis, std, title, x_label, y_label):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.errorbar(x_axis, y_axis, std, linestyle='None', marker='^')\n",
    "    #plt.show()\n",
    "    plt.savefig(title + '.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_graphs(csv_files):\n",
    "    for index, csv_file in enumerate(csv_files):\n",
    "        #Getting dataset information from csv file\n",
    "        df = read_csv(csv_file, ['cross_val', 'kfold','n_trees','accuracy','mean_recall','mean_precision','mean_F1'])\n",
    "        \n",
    "        #Getting global mean and std\n",
    "        mean_metrics, std_metrics = getMeanMetrics(df)\n",
    "        \n",
    "        ##################### Mean metrics vs number of decision Trees #####################\n",
    "        #getting list of ntress\n",
    "        x = [str(x) for x in n_trees]\n",
    "        \n",
    "        #Working over accuracy\n",
    "        accuracy = np.array([x[1] for x in mean_metrics])\n",
    "        acc_std = np.array([y[1] for y in std_metrics])\n",
    "        #Plotting Accuracy\n",
    "        title = 'Accuracy vs n_tree - ' + dataset_names[index]\n",
    "        plot_mean_std(x, accuracy, acc_std, title, 'Numero de árvores', 'Acurácia')\n",
    "        \n",
    "        #Working over recall\n",
    "        recall = np.array([x[2] for x in mean_metrics])\n",
    "        recall_std = np.array([y[2] for y in std_metrics])\n",
    "        #Plotting Accuracy\n",
    "        title = 'Recall vs n_tree - ' + dataset_names[index]\n",
    "        plot_mean_std(x, recall, recall_std, title, 'Numero de árvores', 'Recall')\n",
    "        \n",
    "        \n",
    "        #Working over Precision\n",
    "        precision = np.array([x[3] for x in mean_metrics])\n",
    "        precision_std = np.array([y[3] for y in std_metrics])\n",
    "        #Plotting Accuracy\n",
    "        title = 'Precision vs n_tree - ' + dataset_names[index]\n",
    "        plot_mean_std(x, precision, precision_std, title, 'Numero de árvores', 'Precision')\n",
    "        \n",
    "        #Working over F1-measure\n",
    "        F1 = np.array([x[4] for x in mean_metrics])\n",
    "        F1_std = np.array([y[4] for y in std_metrics])\n",
    "        #Plotting Accuracy\n",
    "        title = 'F1-Measure vs n_tree - ' + dataset_names[index]\n",
    "        plot_mean_std(x, F1, F1_std, title, 'Numero de árvores', 'F1-Measure')\n",
    "        \n",
    "        ##################### Comparing all metrics vs number of decision Trees #####################\n",
    "        plt.title('All metrics vs n_tree - ' + dataset_names[index])\n",
    "        plt.xlabel('Numero de árvores')\n",
    "        plt.ylabel('Metric value')\n",
    "        plt.plot(x, accuracy, '-o')\n",
    "        plt.plot(x, recall, '-^')\n",
    "        plt.plot(x, precision, '-*')\n",
    "        plt.plot(x, F1, '-s')\n",
    "        plt.gca().legend(('accuracy','recall', 'precision', 'F1-Measure'))\n",
    "        #plt.show()\n",
    "        plt.savefig('All metrics vs n_tree - ' + dataset_names[index] + '.png')\n",
    "        plt.close()\n",
    "        \n",
    "        ##################### Metrics vs Cross validation #####################\n",
    "        metrics_cv, std_metrics = getMeanMetricsPerCV(df)\n",
    "        \n",
    "        for n_cross_val, metric_cv in enumerate(metrics_cv):\n",
    "            plt.title('Cross Val. - ' + str(n_trees[n_cross_val]) + ' Arvores-' + dataset_names[index])\n",
    "            plt.xlabel('Cross validation')\n",
    "            plt.ylabel('Metrics')\n",
    "            x_axis = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "            plt.xticks(x_axis)\n",
    "            #accuracy\n",
    "            plt.plot(x_axis, np.array([x[2] for x in metric_cv]), '-o')\n",
    "            #recall\n",
    "            plt.plot(x_axis, np.array([x[3] for x in metric_cv]), '-^')\n",
    "            #precision\n",
    "            plt.plot(x_axis, np.array([x[4] for x in metric_cv]), '-*')\n",
    "            #F1\n",
    "            plt.plot(x_axis, np.array([x[5] for x in metric_cv]), '-s')\n",
    "            plt.gca().legend(('accuracy','recall', 'precision', 'F1-Measure'))\n",
    "            #plt.show()\n",
    "            plt.savefig('Cross Val. - ' + str(n_trees[n_cross_val]) + ' Arvores-' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "        \n",
    "        ##################### Precision, Recall, F1 per class #####################\n",
    "        df2 = read_csv(csv_file)\n",
    "        \n",
    "        if df2.shape[1] == 13:\n",
    "            #only two classes\n",
    "            prec_f1_recalls = getPrecisionRecallPerClass(df2, 2)\n",
    "            \n",
    "            ##################### Recall per Class #####################\n",
    "            plt.title('Recall Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[1] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[4] for x in prec_f1_recalls]), '-^')\n",
    "            plt.gca().legend(('Recall class 1','Recall class 2'))\n",
    "            #plt.show()\n",
    "            plt.savefig('Recall Per Class - ' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "            \n",
    "            ##################### Precision per Class #####################\n",
    "            plt.title('Precision Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[2] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[5] for x in prec_f1_recalls]), '-^')\n",
    "            plt.gca().legend(('Precision class 1','Precision class 2'))\n",
    "            #plt.show()\n",
    "            plt.savefig('Precision Per Class - ' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "            \n",
    "            ##################### F1-Measure per Class #####################\n",
    "            plt.title('F1-measure Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[2] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[6] for x in prec_f1_recalls]), '-^')\n",
    "            plt.gca().legend(('F1-Measure class 1','F1-measure class 2'))\n",
    "            #plt.show()\n",
    "            plt.savefig('F1-measure Per Class - ' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "            \n",
    "        elif df2.shape[1] == 16:\n",
    "            #three classes\n",
    "            prec_f1_recalls = getPrecisionRecallPerClass(df2, 3)\n",
    "            \n",
    "            ##################### Recall per Class #####################\n",
    "            plt.title('Recall Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[1] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[4] for x in prec_f1_recalls]), '-^')\n",
    "            plt.plot(x, np.array([x[7] for x in prec_f1_recalls]), '-*')\n",
    "            plt.gca().legend(('Recall class 1','Recall class 2', 'Recall class 3'))\n",
    "            #plt.show()\n",
    "            plt.savefig('Recall Per Class - ' + dataset_names[index])\n",
    "            plt.close()\n",
    "            \n",
    "            ##################### Precision per Class #####################\n",
    "            plt.title('Precision Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[2] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[5] for x in prec_f1_recalls]), '-^')\n",
    "            plt.plot(x, np.array([x[8] for x in prec_f1_recalls]), '-*')\n",
    "            plt.gca().legend(('Precision class 1','Precision class 2', 'Precision class 3'))\n",
    "            #plt.show()\n",
    "            plt.savefig('Precision Per Class - ' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "            \n",
    "            ##################### Recall per Class #####################\n",
    "            plt.title('F1-measure Per Class - ' + dataset_names[index])\n",
    "            plt.xlabel('Numero de árvores')\n",
    "            plt.ylabel('Metric value')\n",
    "            plt.plot(x, np.array([x[3] for x in prec_f1_recalls]), '-o')\n",
    "            plt.plot(x, np.array([x[6] for x in prec_f1_recalls]), '-^')\n",
    "            plt.plot(x, np.array([x[9] for x in prec_f1_recalls]), '-*')\n",
    "            plt.gca().legend(('F1-Measure class 1','F1-Measure  class 2', 'F1-Measure  class 3'))\n",
    "            #plt.show()\n",
    "            plt.savefig('F1-measure Per Class - ' + dataset_names[index] + '.png')\n",
    "            plt.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_graphs(csv_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
